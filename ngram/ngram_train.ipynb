{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15d9a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "\n",
    "directory = 'datasets/국립국어원 신문 말뭉치(버전 2.0)'\n",
    "\n",
    "json_files = glob.glob(os.path.join(directory, '**', '*.json'), recursive=True)\n",
    "print(len(json_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d370ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "connect = sqlite3.connect('nikl.db')\n",
    "cursor = connect.cursor()\n",
    "\n",
    "count = 0\n",
    "for i, file in enumerate(json_files):\n",
    "    print(i, file)\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        data_file = json.load(f)\n",
    "    for document in data_file['document']:\n",
    "        for paragraph in document['paragraph']:\n",
    "            count += 1\n",
    "            cursor.execute(\"INSERT INTO data VALUES(?, ?)\",(count, paragraph['form']))\n",
    "\n",
    "connect.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ce7b6f",
   "metadata": {},
   "source": [
    "# uni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455ed8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import pad_sequence\n",
    "from nltk.util import bigrams\n",
    "from nltk.util import ngrams\n",
    "from nltk.util import everygrams\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk.lm.preprocessing import flatten\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "\n",
    "n = 1\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31c3ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import MLE\n",
    "uni_model = MLE(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b055050",
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_model.fit(train_data, padded_sents)\n",
    "print(uni_model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a9d01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill as pickle\n",
    "\n",
    "with open('uni_model.pkl', 'wb') as f:\n",
    "    pickle.dump(uni_model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5237f5ba",
   "metadata": {},
   "source": [
    "# Bi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f37af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import MLE\n",
    "from nltk.util import pad_sequence\n",
    "from nltk.util import bigrams\n",
    "from nltk.util import ngrams\n",
    "from nltk.util import everygrams\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk.lm.preprocessing import flatten\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "import dill as pickle\n",
    "\n",
    "n = 2\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, tokenized_text)\n",
    "bi_model = MLE(n)\n",
    "bi_model.fit(train_data, padded_sents)\n",
    "with open('bi_model.pkl', 'wb') as f:\n",
    "    pickle.dump(bi_model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90012bd8",
   "metadata": {},
   "source": [
    "# Tri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b1a14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import defaultdict\n",
    "import sqlite3\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "def calculate_trigram_probabilities(sentence, bigram_counts, trigram_counts):\n",
    "    \"\"\"\n",
    "    Calculate trigram probabilities for a given sentence using loaded bigram and trigram counts.\n",
    "    \"\"\"\n",
    "    # 토큰화된 문장 준비\n",
    "    words = sentence.split()\n",
    "    trigram_probabilities = {}\n",
    "\n",
    "    for i in range(len(words) - 2):\n",
    "        trigram = (words[i], words[i + 1], words[i + 2])\n",
    "        bigram = (words[i], words[i + 1])\n",
    "\n",
    "        # 바이그램과 트라이그램 빈도를 사용하여 확률 계산\n",
    "        if trigram in trigram_counts and bigram in bigram_counts:\n",
    "            trigram_probabilities[trigram] = trigram_counts[trigram] / bigram_counts[bigram]\n",
    "        else:\n",
    "            trigram_probabilities[trigram] = 0\n",
    "\n",
    "    return trigram_probabilities\n",
    "\n",
    "\n",
    "\n",
    "def calculate_ngram_counts(tokenized_text):\n",
    "    \"\"\"\n",
    "    Calculate bigram and trigram counts from tokenized text.\n",
    "    \"\"\"\n",
    "    trigram_counts = defaultdict(int)\n",
    "    bigram_counts = defaultdict(int)\n",
    "\n",
    "    for sentence in tqdm(tokenized_text, desc=\"Processing sentences\"):\n",
    "        for i in range(len(sentence) - 2):\n",
    "            bigram = (sentence[i], sentence[i + 1])\n",
    "            trigram = (sentence[i], sentence[i + 1], sentence[i + 2])\n",
    "            bigram_counts[bigram] += 1\n",
    "            trigram_counts[trigram] += 1\n",
    "\n",
    "    # Remove trigrams that occur only once\n",
    "    trigram_counts = {trigram: count for trigram, count in trigram_counts.items() if count > 1}\n",
    "\n",
    "    return bigram_counts, trigram_counts\n",
    "\n",
    "def save_ngram_counts(bigram_counts, trigram_counts, filename):\n",
    "    \"\"\"\n",
    "    Save bigram and trigram counts to a .pkl file.\n",
    "    \"\"\"\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump((bigram_counts, trigram_counts), f)\n",
    "\n",
    "def load_ngram_counts(filename):\n",
    "    \"\"\"\n",
    "    Load bigram and trigram counts from a .pkl file.\n",
    "    \"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        bigram_counts, trigram_counts = pickle.load(f)\n",
    "    return bigram_counts, trigram_counts\n",
    "\n",
    "# 예시 데이터베이스 경로 (이 경로는 변경해야 함)\n",
    "db_path = 'nikl.db'\n",
    "\n",
    "connect = sqlite3.connect(db_path)\n",
    "cursor = connect.cursor()\n",
    "tokenized_text = [list(map(str, sent[0].split())) for sent in cursor.execute('SELECT input FROM data')]\n",
    "cursor.close()\n",
    "connect.close()\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# tokenized_text = [...]  # This should be your actual tokenized text data\n",
    "bigram_counts, trigram_counts = calculate_ngram_counts(tokenized_text)\n",
    "save_ngram_counts(bigram_counts, trigram_counts, 'tri_model.pkl')\n",
    "probs = calculate_trigram_probabilities('나는 학교에 간다', bigram_counts, trigram_counts)\n",
    "probs  # 문장의 트라이그램 확률 출력\n",
    "# To load the counts later:\n",
    "# bigram_counts, trigram_counts = load_ngram_counts('ngram_counts.pkl')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
