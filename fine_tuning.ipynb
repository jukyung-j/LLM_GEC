{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3dfcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, pipeline\n",
    "from transformers import Trainer, TrainingArguments, AutoModelWithLMHead\n",
    "from copy import deepcopy\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import copy\n",
    "import logging\n",
    "import json\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "## Accerate\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# About tqdm: https://github.com/tqdm/tqdm/#ipython-jupyter-integration\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "# HuggingFace peft 라이브러리\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training, TaskType\n",
    "from transformers import AdamW\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str):\n",
    "    \"\"\"Collects the state dict and dump to disk.\"\"\"\n",
    "    state_dict = trainer.model.state_dict()\n",
    "    if trainer.args.should_save:\n",
    "        cpu_state_dict = {key: value.cpu() for key, value in list(state_dict.items())}\n",
    "        del state_dict\n",
    "        trainer._save(output_dir, state_dict=cpu_state_dict)  # noqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8496609f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    '''Sets the seed of the entire notebook so results are the same every time we run.\n",
    "    This is for REPRODUCIBILITY.'''\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "## Set Seed\n",
    "set_seed(CONFIG['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bec93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "\n",
    "## 모델 준비\n",
    "model_id = \"EleutherAI/polyglot-ko-12.8b\"  # safetensors 컨버팅된 레포\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map='auto')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    padding_side=\"left\",\n",
    "    model_max_length=512,    \n",
    ")\n",
    "\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print(tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d387a829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data config\n",
    "IGNORE_INDEX = -100\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"아래는 작업을 설명하는 명령어와 추가 컨텍스트를 제공하는 입력이 짝을 이루는 예제입니다.\"\n",
    "        \"요청을 적절히 완료하는 응답을 작성하세요.\\n ### 명령어:'''{instruction}'''\\n###  입력:'''{input}'''\\n\\n### 응답:\"\n",
    "    ),\n",
    "     \"prompt_no_input\": (\n",
    "        \"아래는 작업을 설명하는 명령어와 추가 컨텍스트를 제공하는 입력이 짝을 이루는 예제입니다.\"\n",
    "        \"요청을 적절히 완료하는 응답을 작성하세요.\\n ### 명령어:'''{instruction}'''\\n\\n### 응답:\"\n",
    "     ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ea43c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## prepare data\n",
    "from typing import Optional, Dict, Sequence\n",
    "\n",
    "class SFT_dataset(Dataset):\n",
    "    '''SFT dataset by wygo'''\n",
    "    def __init__(self, data_path_1_SFT: str, tokenizer: transformers.PreTrainedTokenizer, verbose=False):\n",
    "        super(SFT_dataset, self).__init__()\n",
    "        logging.warning(\"Loading data...\")\n",
    "        \n",
    "        ## format\n",
    "        pattern_instruction = 'instruction'  # 맞춤법 지시\n",
    "        pattern_input = 'input'  # 입력문\n",
    "        pattern_output = 'output'  # output\n",
    "\n",
    "        ############################################################\n",
    "        ## load dataset\n",
    "        # 내 데이터셋엔 input이 없다\n",
    "#         data_path_1_SFT = 'data_kochatgpt/korean_chatgpt_1_SFT.jsonl'\n",
    "        with open(data_path_1_SFT, \"r\", encoding='utf-8-sig') as json_file:\n",
    "            list_data_dict = json.load(json_file)\n",
    "            if verbose:\n",
    "                print('## data check ##')\n",
    "                print((list_data_dict[0]))\n",
    "\n",
    "        # {'prompt': '불고기용 고기 한우에요?',\n",
    "        #  'completion': \"'저는 인공지능 챗봇이며, 직접적으로 식품에 관한 정보를 가지고 있지 않습니다. 하지만 일반적으로 불고기용 고기는 한우, 쇠고기, 돼지고기 등 다양한 종류의 고기를 사용합니다. 하지만 한우는 대표적인 고급 육류로 알려져 있기 때문에, 한우를 사용하는 경우도 많습니다. 알러지나 개별 건강 상태에 따라 다를 수 있으니 충분한 정보 수집 후에 선택해 주시기 바랍니다.\",\n",
    "        #  'tokens': 193}        \n",
    "\n",
    "        ############################################################\n",
    "        ## 데이터셋 만들기, source와 target\n",
    "        prompt_input, prompt_no_input = PROMPT_DICT[\"prompt_input\"], PROMPT_DICT[\"prompt_no_input\"]  # 템플릿 가져오기\n",
    "\n",
    "        # 입력\n",
    "        sources = []\n",
    "        for example in list_data_dict:\n",
    "            if example.get(pattern_input, \"\") != \"\":\n",
    "                tmp = prompt_input.format_map(example)\n",
    "            else:\n",
    "                tmp = prompt_no_input.format_map(example)\n",
    "            sources.append(tmp)\n",
    "\n",
    "        # 출력\n",
    "        targets = []\n",
    "        for example in list_data_dict:\n",
    "            targets.append(f\"{example[pattern_output]}{tokenizer.eos_token}\")\n",
    "        \n",
    "        if verbose:\n",
    "            idx = 0\n",
    "            print((\"source:\",sources[idx]))\n",
    "            print((\"target:\",targets[idx]))\n",
    "            print(\"Tokenizing inputs... This may take some time...\")\n",
    "\n",
    "        ############################################################\n",
    "        # data_dict = preprocess(sources, targets, tokenizer)  # https://github.com/Beomi/KoAlpaca/blob/04704348d58b8b1c2e2638d6437a04b4e8ba1823/train.py#L124\n",
    "        examples = [s + t for s, t in zip(sources, targets)]\n",
    "\n",
    "        # source data tokenized\n",
    "        sources_tokenized = self._tokenize_fn(sources, tokenizer)  # source만\n",
    "        examples_tokenized = self._tokenize_fn(examples, tokenizer)  # source + target\n",
    "\n",
    "\n",
    "        ## 입력은 source, 출력은 source+target 이지만 학습은 target 부분만\n",
    "        input_ids = examples_tokenized[\"input_ids\"]\n",
    "        labels = copy.deepcopy(input_ids)\n",
    "        for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
    "            label[:source_len] = IGNORE_INDEX  # source 부분은 -100으로 채운다\n",
    "\n",
    "        data_dict = dict(input_ids=input_ids, labels=labels)        \n",
    "        \n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "        logging.warning(\"Loading data done!!: %d\"%(len(self.labels)))    \n",
    "        \n",
    "    def _tokenize_fn(self, strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
    "        \"\"\"Tokenize a list of strings.\"\"\"\n",
    "        tokenized_list = [\n",
    "            tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"longest\",\n",
    "                max_length=tokenizer.model_max_length,\n",
    "                truncation=True,\n",
    "            )\n",
    "            for text in strings\n",
    "        ]\n",
    "        input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
    "        input_ids_lens = labels_lens = [\n",
    "            tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
    "        ]\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            input_ids_lens=input_ids_lens,\n",
    "            labels_lens=labels_lens,\n",
    "        )        \n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object):\n",
    "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )\n",
    "\n",
    "    \n",
    "\n",
    "train_dataset = SFT_dataset(data_path_1_SFT=\"SFT_all_train.json\", tokenizer=tokenizer, verbose=True)\n",
    "eval_dataset  = SFT_dataset(data_path_1_SFT='SFT_all_eval.json', tokenizer=tokenizer, verbose=True)\n",
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "\n",
    "# check\n",
    "print('input : %s'%train_dataset.input_ids[0])\n",
    "print('output: %s'%train_dataset.labels[0])\n",
    "print(tokenizer.decode(train_dataset.input_ids[0]))\n",
    "print(len(train_dataset), len(eval_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd948535",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fea365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QLoRA\n",
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13db3c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training, TaskType\n",
    "\n",
    "# Define LoRA Config\n",
    "lora_config = LoraConfig(r=16,\n",
    "                        lora_alpha=32,\n",
    "                        target_modules=[\"query_key_value\"],\n",
    "                        lora_dropout=0.05,\n",
    "                        bias=\"none\",\n",
    "                        task_type=\"CAUSAL_LM\",\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bd052a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6087bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CONFIG ={\n",
    "    'model': 'EleutherAI/polyglot-ko-12.8b', #'gogamza/kobart-base-v1',\n",
    "    'model_save': 'output/polyglot-ko-12.8b',\n",
    "    'base_path' : './',\n",
    "    'learning_rate': 2e-4, \n",
    "    'seed': 42,\n",
    "    'try': 'injection',\n",
    "    'ratio': 0.95,\n",
    "    'n_sentences' : 10,\n",
    "    \"train_batch_size\": 1, \n",
    "    \"valid_batch_size\": 1, \n",
    "    \"max_length\": 512,\n",
    "    \"target_max_length\": 100,\n",
    "    \"scheduler\": 'CosineAnnealingLR', \n",
    "    \"min_lr\": 1e-6,\n",
    "    \"max_grad_norm\": 1000,\n",
    "    \"T_max\": 500,\n",
    "    \"weight_decay\": 1e-6,\n",
    "    \"n_accumulate\": 2,\n",
    "    'grad_clipping': True,\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f670d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 학습 (10min)\n",
    "import gc\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "# training_args 수정 가능: https://github.com/Beomi/KoAlpaca/blob/main/train.sh 참고\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output/SFT_all\", #The output directory\n",
    "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
    "    num_train_epochs=2, # number of training epochs\n",
    "    per_device_train_batch_size=4, # batch size for training\n",
    "    per_device_eval_batch_size=4,  # batch size for evaluation\n",
    "    learning_rate=CONFIG['learning_rate'],\n",
    "    weight_decay=CONFIG['weight_decay'],\n",
    "    gradient_accumulation_steps=CONFIG['n_accumulate'],\n",
    "    max_grad_norm=CONFIG['max_grad_norm'],\n",
    "    seed=CONFIG['seed'],\n",
    "    load_best_model_at_end=True,\n",
    "    greater_is_better=True,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_total_limit = 1, \n",
    "    report_to=\"wandb\",\n",
    "    save_strategy=\"epoch\",\n",
    "    optim=\"adafactor\",\n",
    "    fp16=True,\n",
    "    resume_from_checkpoint=True\n",
    "    )\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "model.config.use_cache = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17d7e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "run = wandb.init(project='polyglot-all', \n",
    "                 config=CONFIG,\n",
    "                 job_type='Train',\n",
    "                 name = \"ngram\",\n",
    "                 anonymous='must')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb77428e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "## eval\n",
    "trainer.evaluate()\n",
    "model.config.use_cache = True  # silence the warnings. Please re-enable for inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5362829",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained('./output/SFT_aihub')\n",
    "tokenizer.save_pretrained('./output/SFT_aihub')\n",
    "run.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
